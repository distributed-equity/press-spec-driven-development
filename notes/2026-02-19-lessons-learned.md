# 2026-02-19 — Lessons Learned

Session covering specmcp refactor, skillmcp build and refactor, brandmcp
build, audiobook cover proof-of-concept, and author-spec skill creation.
Three MCP servers built, one skill created, one skill spec authored, five
external SDD sources reviewed, and a methodology codified.

## The Workflow Discovery

The biggest outcome of the day was making the SDD workflow explicit. We'd
been doing it all session without naming it. Once named, it became
teachable.

**Two loops, eight steps:**

The Spec Loop (creative work — this is where you spend cognitive budget):

1. Brief — human intent as bullet points, rough shape
2. Spec draft — agent expands brief into a full specification
3. Iterate spec — review, refine, catch gaps
4. Commit spec to main — the spec is the artifact

The Execution Loop (mechanical — should be boring):

5. Plan — agent reads spec, produces execution plan
6. Validate plan against spec — does the plan reveal spec bugs?
7. Execute — the prompt should be minimal
8. Validate results — mandatory checks from the spec

The critical decision gate is step 6. If the plan reveals a spec
deficiency: fix the spec (return to step 3), discard the plan. Do not
patch the plan. The plan is disposable. The spec is the artifact.

We hit this gate three times today — each time, the correct move was to
update the spec, bin the plan, and regenerate.

## Principles Discovered

Every one of these was learned by hitting the problem it prevents.

**The spec is cheap, execution is expensive.**
Rewriting a spec and discarding a plan feels like rework. It is
iteration. The spec is a few hundred words. Execution is files, tests,
commits, reviews. We rewrote the brandmcp spec twice before execution.
Each rewrite was minutes. Execution would have been hours of rework if
done wrong.

**If your prompt needs to explain the work, your spec is deficient.**
The execution prompt should be: `Execute spec <name>`. If you're adding
context, caveats, or instructions, your spec has holes. Fix the spec,
not the prompt.

**Always execute in plan mode first.**
The plan phase validates the spec against the current state of the repo.
Catch problems in the spec, not during execution. The specmcp agent
skipped plan approval and went straight to execution — "the
implementation was done during the planning phase since the plan was
approved implicitly." That's a spec bug, not an agent bug. We added
explicit plan approval to the workflow.

**When the plan reveals a spec bug, fix the spec — not the plan.**
The SKILL.md naming collision was caught at plan time. The plan correctly
identified that `get_skill("SKILL")` would be ambiguous with two skills.
The fix was in the spec (rename files to `<skill-name>.md`), not in the
plan.

**Don't burn cognitive budget on error recovery.**
If the agent encounters issues during execution, the solution is to
debug and improve the specification, not manually fix outputs.

**The bug is always in the spec.**
When execution produces wrong results, look at the spec first. Ambiguity,
missing requirements, and implicit assumptions are spec bugs, not agent
bugs.

**State conventions explicitly.**
Never assume an agent knows a convention. The skillmcp refactor found
missing provenance file filtering — a convention that existed in specmcp
but wasn't stated in any skill or spec. If provenance files should be
overwritten not appended, say so. If tools must return error strings not
raise exceptions, say so.

**Single source of truth — always. Never defer unification.**
The brandmcp spec initially had design tokens defined both in a JSON
file and hardcoded in markdown. Plan review caught this. We unified to
`tokens.json` as the single canonical source before execution — didn't
defer to a "future spec." If you know two sources of truth is wrong, fix
it now.

**Out of scope prevents gold-plating.**
Without explicit boundaries, agents expand scope. They update
documentation you didn't ask for, refactor code that wasn't in the spec,
and add features that seemed related. Every spec needs an explicit "Out
of Scope" section.

**Mandatory means mandatory.**
The brandmcp plan reduced the spec's 14 verification checks to "ruff,
format, pre-commit, server starts." That's the plan failing the spec.
Write: "Every check below is mandatory. Do not skip any." Include
positive and negative tests.

**Provenance is a side effect, not extra work.**
Provenance files are generated as part of spec execution. They cost
almost nothing and become invaluable as audit trails, case studies, and
primary source material for the book. The best documentation is a side
effect of the process, not an additional task.

**The brief is the spec for the spec.**
New layer discovered: brief → spec → plan → execution. The brief
captures human intent as bullet points. The spec is the precise,
complete document an agent can execute without further context. This
distinction matters because the brief is where you think, the spec is
where you hand off.

**If you're not sure about the spec, execute two and pick the winner.**
When reviewing a spec and you can't tell whether it's good enough —
execute two versions on different branches and compare the outputs. The
spec is cheap. Execution is cheap. The delta between results tells you
exactly what matters in the spec and what was noise. We did this with
the readme-update spec: v1 was prescriptive (16 subsections dictating
each paragraph), v2 was principled (5 steps, intent over implementation).
Comparing the two would have told us precisely how much per-section
guidance the executing agent actually needs. Don't theorise about spec
quality — run the experiment. Keep the spec that produced the better
result, bin the other.

**The whole execution is three prompts.**
The readme-update execution proved this. Prompt 1: "create a plan."
Prompt 2: a three-line correction after plan review (fix the generation
notice wording, acknowledge the MCP fallback deviation, fix a misleading
verification check). Prompt 3: "execute." Total cognitive budget spent
on reviewing the plan and writing three sentences. Everything else was
mechanical. If your execution takes more than three prompts, your spec
has holes or your plan review isn't catching enough.

**Plan review earns its keep on the first run.**
The readme-update plan had the generation notice wrong — a generic
"DO NOT HAND-EDIT" comment instead of the spec-mandated wording that
names the spec and says "update the spec instead." Without plan review,
the README would have shipped with a generation notice that didn't tell
anyone which spec to update. That's not a hypothetical — it was a
concrete bug caught in sixty seconds of reading the plan, fixed with
one sentence in the correction prompt. The step 6 decision gate isn't
ceremony; it's where you catch the bugs that would cost you a whole
re-execution to fix.

**Log deviations, don't hide them.**
The readme-update execution couldn't call MCP tools (environment
constraint), so the agent fell back to reading Python source files. The
brief explicitly said "do NOT read Python source files." Rather than
pretending this didn't happen, the plan logged it as a known deviation,
and the provenance file records it. Deviations aren't failures — hidden
deviations are. If you know you're going off-spec, say so. The
provenance file is the place to be honest. Future readers (including
yourself) will thank you.

**Some verification needs eyes, not checks.**
The epigraph spec had a thorough verification checklist — file exists,
LaTeX environment defined, CSS rules present, no gold-plating. Every
check passed. But none of them could tell you whether the epigraph
actually looks right on a 6×9 inch page. Is the vertical centering
balanced? Is `\large` the right size for the quote? Does 70% text width
feel right? That's not a spec deficiency — it's a category of
validation that can't be automated. The spec defines the structure; the
human confirms the aesthetics. When visual tweaks are needed, they go
back into the spec (update the LaTeX values, re-execute), not into the
files. The spec stays the source of truth even for intuitive judgements.

**Not everything is a spec.**
The preface is the author talking directly to the reader. There's
nothing to discover, nothing to generate from data, nothing an agent
needs to figure out. It's pure editorial judgement — the one thing SDD
explicitly doesn't automate. Just write it, commit it, move on. No
spec, no skill, no provenance. The build pipeline picks it up
automatically. If you start writing specs for things that are entirely
human creative output, you've crossed from methodology into cargo cult.
The question is always: "Is there work here that benefits from
specification?" If the human is the entire loop — intent, creation,
and validation — a spec adds ceremony, not value.

**The unbroken chain is the proof.**
Visitor hits the repo. The generated README tells them it's spec-driven.
One click to sddbook.com. One click to download the PDF. That PDF was
built by CI/CD from the same repo they're looking at. The build pipeline
was created from specs. The specs are listed in the README they just
read. And the commit hash on the repo page is the same commit hash on
the copyright page of the book they just downloaded. Full provenance
from source to artifact, verifiable by anyone. There's no point in the
chain where it stops being SDD and becomes "trust me, the methodology
works." The repo is the proof. The book is the case study. The README
is the demo. The commit hash is the receipt.

## New IP Created This Session

We reviewed five external SDD sources before authoring the skill:

- Birgitta Böckeler (Fowler) — SDD levels taxonomy, Oct 2025
- InfoQ — architecture as executable, drift detection, Jan 2026
- Microsoft — spec-kit, constitution concept, Sep 2025
- GitHub spec-kit — Specify → Plan → Tasks workflow
- Thoughtworks — SDD as fifth-generation abstraction

**What none of them cover that we developed today:**

**Brief-to-spec expansion as a first-class step.** None of the reviewed
tools explicitly model the human intent capture step. Kiro goes straight
to "Requirements." Spec-kit starts at "Specify." Nobody treats the
brief — the messy, incomplete, bullet-point version of intent — as a
distinct, valuable phase that precedes the spec.

**Spec iteration as the primary creative act.** Spec-kit treats specs as
something you write once and execute. We proved that spec revision *is*
the work. Rewriting the brandmcp spec twice before execution wasn't
rework — it was the creative process. The execution was mechanical.

**The two-loop workflow with a decision gate.** No tool models the
explicit separation between the spec loop (creative) and the execution
loop (mechanical), with the plan-as-spec-validator decision gate at step
6. Kiro has Requirements → Design → Tasks. Spec-kit has Specify → Plan
→ Tasks. Neither has the explicit "does the plan reveal a spec bug?"
gate that sends you back to the spec.

**Provenance as reusable content pipeline.** Nobody else is capturing
execution records as primary source material. The provenance files are
not just audit trails — they're case studies, chapter content, and
masterclass training material. Every spec execution generates a
provenance record documenting how the book was built using SDD. The book
teaches SDD. The provenance files prove SDD works. The masterclass
teaches people how to do SDD using the same artifacts. Turtles all the
way down.

**Git history as narrative arc.** Because provenance files are
overwritten (not appended), `git log --follow <file>.provenance.md`
gives the full evolution — spec changes, execution changes, what broke,
what got fixed. A narrative arc built into version control. Not designed,
discovered.

## Bugs Caught by the Workflow

**Missing `validate_brand` tool.**
The original brandmcp spec had three tools. During plan review, we
realised there was no validation tool — the server could serve brand
data but couldn't check content against it. Spec updated, plan binned,
re-executed.

**Shallow verification in plans.**
The brandmcp plan reduced 14 mandatory checks to "ruff, format,
pre-commit, starts." The spec was reinforced with explicit language:
"Every check below is mandatory. Do not skip any."

**Two sources of truth for design tokens.**
The brandmcp spec initially had design values in both `tokens.json` and
hardcoded in markdown files. Plan review caught the duplication. Spec
updated: `tokens.json` is canonical, markdown provides narrative context
only.

**SKILL.md naming collision.**
The author-spec plan revealed that all skills sharing the filename
`SKILL.md` would collide in `load_skill()`, which matches on file stem.
`get_skill("SKILL")` would return the alphabetically-first match —
`author-spec`, not `mcp-builder`. Fix: each skill file named after the
skill itself (`mcp-builder.md`, `author-spec.md`). Spec updated to
include the rename.

**Agent skipping plan approval.**
The specmcp refactor agent executed during the planning phase, treating
"no objection" as "go ahead." This was a spec bug — AGENTS.md didn't
explicitly require plan approval before execution. Added: "Do not
execute until the plan is explicitly approved by the author."

**Missing provenance file filtering in skillmcp.**
The skillmcp server didn't filter `.provenance.md` files from discovery
or loading — a convention that existed in specmcp but wasn't applied
when skillmcp was built before the mcp-builder skill existed. The skill
now documents this convention explicitly.

**Wrong generation notice in readme-update plan.**
The plan used a generic `<!-- GENERATED FILE — DO NOT HAND-EDIT -->`
instead of the spec-mandated wording that names the spec and tells the
reader which file to update. Plan review caught it. The fix was one
sentence in the correction prompt: "Generation notice must match the
spec — use the spec name and 'update the spec instead' message." The
plan was updated, not the spec — because this was a plan bug, not a
spec bug. The spec was clear; the agent just didn't follow it precisely.

**Agent didn't start MCP servers — because the spec didn't say to.**
The readme-update spec said "use MCP tools to discover" but the agent
declared MCP servers "unavailable" and fell back to reading Python
source files — the exact fallback the brief said not to use. The
servers are just Python scripts; the agent could have read `.mcp.json`,
started each server, called the list tools, and shut them down. It
didn't, because the spec assumed MCP tools would be available without
specifying what "available" means or how to make them available. The
agent took the path of least resistance. Classic "the bug is always in
the spec" — the spec left a gap, the agent filled it with a reasonable
but off-spec workaround. Fix: one line in the spec — "If MCP servers
are not running, start them using the entry points in `.mcp.json`
before running discovery."

## The Ouroboros

The recursive self-reference continued all session:

- Used SDD to write the first spec for a spec about SDD
- Used a skill to build the skill server that serves skills
- The mcp-builder skill lives inside the server it describes how to build
- Used a spec to create a skill about writing specs
- The book about SDD is built using SDD
- The provenance files generated by SDD become book content about SDD
- A future masterclass will teach SDD using the provenance, specs, and
  skills generated while building the book that describes SDD
- The commit hash on the repo is the commit hash on the book's copyright
  page — the artifact proves its own provenance

"It's turtles all the way down."

## Proof Runs

**Brand server as proof of concept.** An agent with no prior knowledge
of the book's visual identity called three tools (`get_design_tokens`,
`get_brand("palette")`, `get_brand("typography")`), got structured data
back, and produced a brand-compliant audiobook cover on the first
attempt. No iterations, no corrections, no "actually the red is
#e63926." Every colour, font, and proportion sourced from `tokens.json`.

This validates the full chain:
- specmcp — specs drive the work
- skillmcp — skills teach agents how to build
- brandmcp — brand data is queryable and consumable

## Spec Categories Established

- `devops/` — infrastructure specs (specmcp, skillmcp, brandmcp)
- `editorial/` — book content specs
- `methodology/` — process and practice specs (author-spec)

The author-spec was initially placed in `devops/`. Corrected to
`methodology/` — it's a process spec, not an infrastructure spec.

## Anti-Patterns Identified

**The novel spec.** Reads like a design document — pages of context and
rationale with requirements buried in prose. Specs are instructions, not
essays.

**The implicit spec.** Assumes the agent knows things it hasn't been
told. "Follow our usual conventions" is not a requirement.

**The mega-spec.** Tries to do too much. If a spec has more than ~10
changes, it probably wants to be two specs.

**The unverifiable spec.** No verification section, or vague checks like
"ensure it works." If you can't write a concrete check, you don't know
what success looks like.

**The deferred debt spec.** Knowingly creates a problem and adds "future
spec" to fix it. If you know it's wrong now, fix it now.

**The prompt-dependent spec.** Only works if the execution prompt adds
extra context. Test: can a fresh agent with no prior conversation
execute from the spec alone?

## External Sources — Key Takeaways

**Böckeler's taxonomy (Fowler, Oct 2025):**
Three SDD levels: spec-first (write, use, discard), spec-anchored (keep
as living doc), spec-as-source (humans only edit specs, never code). Our
project is spec-as-source. Useful framing to position the book.

**Böckeler's spec vs memory bank distinction:**
Memory bank = project-wide context (AGENTS.md, skills, brand). Specs =
task-specific documents. Don't duplicate — reference.

**Böckeler's verification critique:**
"Your role isn't just to steer. It's to verify." Specs must have
explicit, enumerated checks. Maps directly to our "mandatory means
mandatory" principle.

**InfoQ — architecture as executable:**
SDD as "fifth-generation abstraction." Drift detection — the spec
remains the source of truth, the system detects when code has drifted.
Our `validate_brand` and `validate_content` are exactly this.

**Microsoft/spec-kit — the constitution concept:**
Immutable project-wide principles separate from individual specs. Our
AGENTS.md + skills serve this role. Specs reference but don't duplicate
constitutional knowledge.

**Spec-kit — workflow phases:**
Specify → Plan → Tasks, with checklists as "definition of done" for
each phase. Maps to our Brief → Spec → Plan → Execute → Validate.

## Files Created This Session

**MCP Servers:**
- `.brandmcp/server.py` (4 tools: list_brand, get_brand,
  get_design_tokens, validate_brand)
- `.brandmcp/brand/tokens.json` (canonical design token source)
- `.brandmcp/brand/palette.md`, `typography.md`, `layout.md`, `voice.md`

**Skills:**
- `.skillmcp/skills/mcp-builder/SKILL.md` (created, later marked for
  rename to `mcp-builder.md`)

**Specs:**
- `.specmcp/specs/devops/brandmcp-build.md`
- `.specmcp/specs/methodology/author-spec-skill.md`

**Provenance:**
- `.specmcp/specs/provenance/devops/specmcp-refactor.provenance.md`
- `.specmcp/specs/provenance/devops/skillmcp-refactor.provenance.md`
- `.specmcp/specs/provenance/devops/brandmcp-build.provenance.md`

**Other:**
- `audiobook-cover-preview.svg` / `.png` (brand server proof of concept)
- `AGENTS.md` (updated with all three MCP servers)

## Compounding Velocity

The most important empirical observation from the session: development
velocity increased measurably as each MCP foundation layer went in.

The first server (specmcp) was hand-built with no skill, no conventions,
no pattern to follow. It worked, but every decision was made from
scratch.

The second server (skillmcp) was built from the mcp-builder skill — but
the skill didn't exist when the server was first created, so it needed a
refactor pass to align with conventions. Two steps: build, then fix.

The third server (brandmcp) was built entirely from spec by an agent on
the first attempt. The skill taught the agent how to build. The spec
told it what to build. It produced a working four-tool MCP server, and
minutes later an agent used that server to produce a brand-compliant
audiobook cover with zero human intervention on the creative output.
First attempt, no corrections.

Each layer makes the next layer faster:

- The skill teaches agents how to build (no more rediscovering
  conventions)
- The spec tells agents what to build (no more ambiguous prompts)
- The brand server gives agents the tokens to build with (no more
  hunting for hex codes)
- Provenance captures what happened so the next spec is better
  (institutional memory)
- The author-spec skill closes the loop — now even the specs themselves
  have a skill teaching agents how to write them

This is the thesis of the book playing out in real time. The spec is
cheap, the infrastructure is cheap, and the velocity compounds. This
isn't a theoretical argument for SDD — it's empirical evidence observed
across a single working session. The first server took the longest. The
third server was built and validated in a fraction of the time. The
fourth deliverable (audiobook cover) required no spec at all — just a
natural language prompt and three tool calls.

The investment in specifications, skills, and structured data pays back
faster than expected, because each artifact reduces the cognitive load
for every subsequent task.

**The IDE disappeared.** By the last few commits of the session, the
author stopped opening an IDE entirely. No diff review, no file
scanning, no manual structure checks. The specs define what to verify.
The mandatory checks confirm correctness. The provenance documents
decisions. `git log` provides the audit trail. The entire workflow —
spec authoring, plan review, execution approval, validation — happened
through conversation and agent execution. The IDE is for code. When
code is a side effect, you don't need one.

## Quotes Worth Keeping

"The best documentation is a side effect of the process, not an
additional task."

"If your prompt needs to explain the work, your spec is deficient."

"The spec is cheap, execution is expensive."

"When the plan reveals a spec bug, fix the spec — not the plan."

"`git log --follow` on overwritten provenance files gives the full
evolution — spec changes, execution changes, what broke, what got fixed.
A narrative arc built into version control."

"The agent skipped plan approval and went straight to execution. That's
a spec bug, not an agent bug."

"Each layer makes the next layer faster. The spec is cheap, the
infrastructure is cheap, and the velocity compounds."

"The IDE is for code. When code is a side effect, you don't need one."

"If you're not sure about the spec, execute two and pick the winner.
Don't theorise about spec quality — run the experiment."

"The whole execution is three prompts. If it's more, your spec has
holes."

"Deviations aren't failures. Hidden deviations are."

"The step 6 decision gate isn't ceremony — it's where you catch the
bugs that would cost you a whole re-execution to fix."

"The spec defines the structure. The human confirms the aesthetics."

"Not everything is a spec. If the human is the entire loop, a spec
adds ceremony, not value. Don't cargo-cult the methodology."

"The repo is the proof. The book is the case study. The README is the
demo. The commit hash is the receipt."

"The spec said 'use MCP tools' but didn't say 'start the servers.' The
agent shrugged and read the source files instead. The bug is always in
the spec."

"It's turtles all the way down."

## What's Next

1. Execute author-spec skill (spec updated with SKILL.md rename fix,
   plan stale — needs fresh session and new plan)
2. Merge all PRs (specmcp, skillmcp, brandmcp)
3. Continue book chapters
4. Future: create SDD masterclass from provenance files, specs, and
   skills — after a few chapters are down
